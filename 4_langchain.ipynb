{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZf6Qke31NHBlq8cdW2kF5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThisIsFarhan/Language_Models/blob/main/4_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mupzq4uIbm_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac6a4b2-1396-419a-bf84-4a6a2ca86847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting llama-cpp-python==0.2.69\n",
            "  Downloading llama_cpp_python-0.2.69.tar.gz (42.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.69)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.69) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain>=0.1.17 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 langchain_community\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python==0.2.69"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAGg4R-Eby7Q",
        "outputId": "d9a4f754-a684-4a51-a131-d9f2bf5dd6be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-06 12:57:28--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.61, 3.165.160.11, 3.165.160.12, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.61|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1738850248&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODg1MDI0OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=aBu9kfQgJZlx6A0pJxTo9tXe8e1zjWjUYnQC6cqtFTRzqWaAoUgu-05ocHtb1cUam8FX2EUOcO4Lag6ZrykhGp12v4oXmoyUCSifazRUudpGe8ciemMiYULxYdVGEWc%7EkLXcYhh0xYM%7EcC3px7HQFBk3bxx6Fynw5%7EDkcWV0TwMnQNEw7LzKd2spuyNqGehrXl2VYkeBobsRfXD%7E-nT4uuiEEbXfubQE%7EkHRwldT-3W%7EBwjas0xvcepD3XXV09rNU2Jni59O%7ExYh-qQbjgjl7-YBgnuL3sijjIl4Ra4qGXgXKuDO%7EA-9zCcYDDClMpPKyqMaqpp5f6m1uMKyNcmG7Q__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-02-06 12:57:28--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1738850248&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODg1MDI0OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=aBu9kfQgJZlx6A0pJxTo9tXe8e1zjWjUYnQC6cqtFTRzqWaAoUgu-05ocHtb1cUam8FX2EUOcO4Lag6ZrykhGp12v4oXmoyUCSifazRUudpGe8ciemMiYULxYdVGEWc%7EkLXcYhh0xYM%7EcC3px7HQFBk3bxx6Fynw5%7EDkcWV0TwMnQNEw7LzKd2spuyNqGehrXl2VYkeBobsRfXD%7E-nT4uuiEEbXfubQE%7EkHRwldT-3W%7EBwjas0xvcepD3XXV09rNU2Jni59O%7ExYh-qQbjgjl7-YBgnuL3sijjIl4Ra4qGXgXKuDO%7EA-9zCcYDDClMpPKyqMaqpp5f6m1uMKyNcmG7Q__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.165.160.20, 3.165.160.38, 3.165.160.3, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.165.160.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7643295904 (7.1G) [binary/octet-stream]\n",
            "Saving to: ‘Phi-3-mini-4k-instruct-fp16.gguf’\n",
            "\n",
            "i-4k-instruct-fp16.   2%[                    ] 156.08M  40.7MB/s    eta 2m 56s ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LlamaCpp\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=2048,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "2lks9uxjcy-z",
        "outputId": "7272e779-6de6-4ef8-c825-74b311e58826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_community'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-746052f0fb2f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlamaCpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Make sure the model path is correct for your system!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m llm = LlamaCpp(\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Phi-3-mini-4k-instruct-fp16.gguf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mHuggingFaceTextGenInference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LlamaCpp\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlamaCpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0m_warn_on_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"langchain_community.llms.LlamaCpp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "hIshxnBadsJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chains"
      ],
      "metadata": {
        "id": "dCvThFdgcesf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Single chain\n",
        "\n",
        "\n",
        "template = \"\"\"<s><|User|>\n",
        "{input_prompt}<|END|>\n",
        "<|Assistant|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")\n",
        "\n",
        "basic_chain = prompt | llm\n",
        "basic_chain.invoke({\n",
        "    \"input_prompt\":\"Tell me about computer engineering\"\n",
        "})"
      ],
      "metadata": {
        "id": "6JWrKE0Zc5v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Multiple chain\n",
        "from langchain import LLMChain\n",
        "\n",
        "#--------------------Templates------------------------------\n",
        "summary_template = \"\"\"<s><|user|>\n",
        "Create a title for a story about {summary}. Only return the one-liner title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "summary_prompt = PromptTemplate(template=summary_template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=summary_prompt, output_key=\"title\")\n",
        "\n",
        "title_template = \"\"\"<s><|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "title_prompt = PromptTemplate(\n",
        "    template=title_template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "character = LLMChain(llm=llm, prompt=title_prompt, output_key=\"character\")\n",
        "\n",
        "story_template = \"\"\"<s><|user|>\n",
        "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "story_prompt = PromptTemplate(\n",
        "    template=story_template, input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")\n",
        "\n",
        "\n",
        "#Defining the chain\n",
        "llm_chain = title | character | story\n",
        "\n",
        "\n",
        "#invokation\n",
        "llm_chain.invoke(\"A person developing an efficient AI agent\")"
      ],
      "metadata": {
        "id": "n9-wDt-_dZxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Memory"
      ],
      "metadata": {
        "id": "Z0B1DEUHg-9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Conversation Buffer\n",
        "\n",
        "from langchain import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "template = \"\"\"<s><|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n"
      ],
      "metadata": {
        "id": "4dS3wBpuhAX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"Hi I am Farhan\"})"
      ],
      "metadata": {
        "id": "BtSwIUkzim80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"Whats my name?\"})"
      ],
      "metadata": {
        "id": "97OFKmQjiuLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conversation Buffer Window\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "nBJwIk0Ai-DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"Hi My name is Farhan\"})"
      ],
      "metadata": {
        "id": "oQG4_K2wmQlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"Tell me a joke\"})"
      ],
      "metadata": {
        "id": "L0_jJArlmVhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is 2+2?\"})"
      ],
      "metadata": {
        "id": "5WZv5l1LmZPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is the mean of AGI?\"})"
      ],
      "metadata": {
        "id": "Y8izWSU0md6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"what is my name?\"})"
      ],
      "metadata": {
        "id": "d8bUFwVZmsc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Summary Buffer\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "summary_prompt_template = \"\"\"<s><|user|>Summarize the conversation and update it with new lines\n",
        "\n",
        "Currenty Summary:\n",
        "{summary}\n",
        "\n",
        "New Lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New Summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    llm = llm,\n",
        "    template=summary_prompt_template,\n",
        "    input_variables=[\"new_lines\",\"summary\"]\n",
        ")\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",prompt=summary_prompt)\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "uCpRx7kBnTPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"Hi My name is Farhan\"})"
      ],
      "metadata": {
        "id": "uK2OBS879ETC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is MQTT?\"})"
      ],
      "metadata": {
        "id": "CG7FqKAA9R8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is AGI?\"})"
      ],
      "metadata": {
        "id": "PTp07Brl94uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
      ],
      "metadata": {
        "id": "bI816o5R-CWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ReAct Agent"
      ],
      "metadata": {
        "id": "46ySapVXUszv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_groq duckduckgo-search>=5.2.2 langchain>=0.1.17 langchain_community"
      ],
      "metadata": {
        "id": "tKpyDynxVB-z",
        "outputId": "faf13313-6cf8-4435-edbe-2a1032cd7f08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusolver-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "groq_api_key=userdata.get('groq_api_key')\n",
        "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Gemma2-9b-It\")"
      ],
      "metadata": {
        "id": "Wu10VzsBUxIL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=react_template, input_variables=[\"tools\", \"tool_names\",\"input\",\"agent_scratchpad\"])"
      ],
      "metadata": {
        "id": "5LXjyWnPVJAb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools, Tool\n",
        "from langchain.tools import DuckDuckGoSearchResults\n",
        "\n",
        "search = DuckDuckGoSearchResults()\n",
        "search_tool = Tool(name=\"ducksearch\",description=\"A web search engine. Use this to as a search engine for general queries.\",func=search.run)\n",
        "tools = load_tools([\"llm-math\"],llm=llm)\n",
        "tools.append(search_tool)"
      ],
      "metadata": {
        "id": "RfHAY949WrT2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "id": "0q0HrP8SXA34",
        "outputId": "e280790a-730a-443f-82e9-a0df45e93de6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7b8ff13f8390>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7b8ff13db390>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}))>, coroutine=<bound method Chain.arun of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7b8ff13f8390>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7b8ff13db390>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}))>),\n",
              " Tool(name='ducksearch', description='A web search engine. Use this to as a search engine for general queries.', func=<bound method BaseTool.run of DuckDuckGoSearchResults(api_wrapper=DuckDuckGoSearchAPIWrapper(region='wt-wt', safesearch='moderate', time='y', max_results=5, backend='auto', source='text'))>)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handling_parsing_errors=True)\n"
      ],
      "metadata": {
        "id": "DgOItGNMXC1O"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\":\"Tell me what KiloGrams actually means and convert 2.45kg to grams\"\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "Yvsft3skbrfF",
        "outputId": "7b2f6b56-e5e0-4ce0-a3e2-946d720a9a38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to look up the definition of Kilograms and how it relates to grams. Then I can use a calculator to perform the conversion. \n",
            "Action: ducksearch\n",
            "Action Input: What is a Kilogram? How many grams are in a kilogram?\u001b[0m\u001b[33;1m\u001b[1;3msnippet: One kilogram is equal to 1,000 grams. In the metric system, the unit of mass is the gram (g), and the prefix \"kilo-\" denotes a factor of 1000. Therefore, when converting from kilograms to grams, 1 kilogram is equivalent to 1000 grams. The conversion can be expressed mathematically as follows: 1 kilo, title: 1 kilogram is equal to grams. - GeeksforGeeks, link: https://www.geeksforgeeks.org/1-kilogram-is-equal-to-grams/, snippet: One kilogram is equal to 1000 grams, creating a ratio of 1:1000 between the two units. This means that one kilogram is 1000 times larger than one gram. For example, if you have 500 grams, it would be equivalent to 0.5 kilograms since you divide the grams by 1000. Conversely, if you have 2 kilograms, it would be equal to 2000 grams, as you ..., title: Converting Weight: How Many Grams in a Kilogram, link: https://www.measuringknowhow.com/converting-weight-how-many-grams-in-a-kilogram/, snippet: Conversion Factor: 1000 grams/kilogram. Weight in Grams: 5 kg * 1000 grams/kilogram = 5000 grams. Therefore, 5 kilograms is equivalent to 5000 grams. This example demonstrates the process of converting from kilograms to grams using the conversion factor of 1000 grams per kilogram., title: Converting Kilograms: How Many Grams in a KG?, link: https://www.measuringknowhow.com/converting-kilograms-how-many-grams-in-a-kg/, snippet: One kilogram is equal to 1,000 grams. In the metric system, the unit of mass is the gram (g), and the prefix \"kilo-\" denotes a factor of 1000. Therefore, when converting from kilograms to grams, 1 kilogram is equivalent to 1000 grams. The conversion can be expressed mathematically as follows: 1 kilogram = 1000 grams, title: 1 kg equals to how many grams? - GeeksforGeeks, link: https://www.geeksforgeeks.org/1-kg-equals-to-how-many-grams/\u001b[0m\u001b[32;1m\u001b[1;3mThought: I know that 1 kilogram is equal to 1000 grams.  I can multiply 2.45 kilograms by 1000 to find the equivalent in grams. \n",
            "Action: Calculator\n",
            "Action Input: 2.45 * 1000\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 2450.0\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer\n",
            "Final Answer: 2.45 kilograms is equal to 2450 grams.  \n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Tell me what KiloGrams actually means and convert 2.45kg to grams',\n",
              " 'output': '2.45 kilograms is equal to 2450 grams.'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "olShQEQcb1Ec"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}