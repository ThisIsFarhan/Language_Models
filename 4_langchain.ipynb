{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNWbsNu91WELevlAwq27UQR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThisIsFarhan/Language_Models/blob/main/4_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mupzq4uIbm_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a54729-6ff0-485b-a489-7091eb8691fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting llama-cpp-python==0.2.69\n",
            "  Downloading llama_cpp_python-0.2.69.tar.gz (42.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.69)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.69) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.69-cp311-cp311-linux_x86_64.whl size=55713428 sha256=c214a92e1eea1cbd5c6d433fa616717c1193b52b9664217610e0805433370819\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/1b/ff/b4dba97fbd16e731705b262602ba8f3b672bf4bde54ea0c104\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.69\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain>=0.1.17 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 langchain_community\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python==0.2.69"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAGg4R-Eby7Q",
        "outputId": "307b9182-9348-42a1-e4fd-b85c8957f2ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-31 19:57:08--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.121, 13.35.202.34, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1738357028&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODM1NzAyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=JzMQqqeYW3Z%7E2WpLtRRpdkq7P8DDhtzgN3OrpFW5FLrSWdpQ6jAtpBgyz67ldDBaHuJehq50tRZ6RWLFS0M0wH9EJpbGKqJ6MDrqUFlCmXEcKvPCjYiJx6dIqH4FfLPrXhWSUg2HwAF0nsRPa1gPeqJR7iUKoXxlrhfGwgGDOEId-mUsmCd9CODx6US5i0zkHusSCtJWVzS470KFJUGxng0fZf6soeT-ac%7Eo4lk74gTVqAXk3TioP9XbYNbI27njrEGkP4mIwoKx42sgjkm%7Eh7Xyp0h5tGyLiLOM-hjAq-Z4Zbt8gDthC9B0DpeI8JO%7EDRn-a2tBtSuh6MNwx6%7ELcA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-01-31 19:57:08--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1738357028&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODM1NzAyOH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=JzMQqqeYW3Z%7E2WpLtRRpdkq7P8DDhtzgN3OrpFW5FLrSWdpQ6jAtpBgyz67ldDBaHuJehq50tRZ6RWLFS0M0wH9EJpbGKqJ6MDrqUFlCmXEcKvPCjYiJx6dIqH4FfLPrXhWSUg2HwAF0nsRPa1gPeqJR7iUKoXxlrhfGwgGDOEId-mUsmCd9CODx6US5i0zkHusSCtJWVzS470KFJUGxng0fZf6soeT-ac%7Eo4lk74gTVqAXk3TioP9XbYNbI27njrEGkP4mIwoKx42sgjkm%7Eh7Xyp0h5tGyLiLOM-hjAq-Z4Zbt8gDthC9B0DpeI8JO%7EDRn-a2tBtSuh6MNwx6%7ELcA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.33.88.42, 13.33.88.68, 13.33.88.63, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.33.88.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7643295904 (7.1G) [binary/octet-stream]\n",
            "Saving to: ‘Phi-3-mini-4k-instruct-fp16.gguf’\n",
            "\n",
            "Phi-3-mini-4k-instr 100%[===================>]   7.12G  40.7MB/s    in 3m 2s   \n",
            "\n",
            "2025-01-31 20:00:11 (40.0 MB/s) - ‘Phi-3-mini-4k-instruct-fp16.gguf’ saved [7643295904/7643295904]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LlamaCpp\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=2048,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "2lks9uxjcy-z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "hIshxnBadsJm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chains"
      ],
      "metadata": {
        "id": "dCvThFdgcesf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Single chain\n",
        "\n",
        "\n",
        "template = \"\"\"<s><|User|>\n",
        "{input_prompt}<|END|>\n",
        "<|Assistant|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")\n",
        "\n",
        "basic_chain = prompt | llm\n",
        "basic_chain.invoke({\n",
        "    \"input_prompt\":\"Tell me about computer engineering\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "6JWrKE0Zc5v0",
        "outputId": "d3f03555-953b-48e7-f0bd-a04ddb25fc99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Computer Engineering is an interdisciplinary branch of engineering that integrates principles from electrical engineering and computer science. It's focused on the design, development, testing, and manufacturing of both hardware (physical parts) and software for computers, servers, storage devices, mobile phones, tablets, embedded systems in appliances and vehicles etc.\\n\\nComputer engineers work to develop new technology that makes existing devices faster or smaller but more powerful, as well as developing entirely new technologies like quantum computing. They also focus on improving the security of these technologies and designing efficient algorithms for processing data.\\n\\nThe field is continually evolving due to rapid advancements in digital technology. Computer engineering requires a solid understanding of both hardware and software elements, so many professionals hold degrees in electrical or computer science. \\n\\nKey sub-fields within computer engineering include:\\n\\n1. Embedded Systems Engineering: This involves designing systems that are part of larger devices but have specialized functions (like sensors for a smartphone).\\n2. Computer Architecture: The process of designing the structure and organization of a computer system, including its hardware components like CPUs and memory units.\\n3. Digital Signal Processing: This area involves manipulation of signals (like audio or video data) in digital format to enhance quality or extract information.\\n4. Telecommunication Engineering: Involves the transmission of information over distances for use in computing systems, such as cellular networks or internet services.\\n5. Network and Computer Security: This is about securing computer systems from cyber threats through various techniques like cryptography and intrusion detection systems.\\n6. Software Development: Computer engineers also work on creating software that runs the hardware components of a system, such as operating systems or applications.\\n\\nIn short, computer engineering involves a combination of electrical engineering to create and improve physical devices, and information technology knowledge for programming and designing complex software systems. \\n\\nThe field is vast with numerous opportunities in research, development, product management, quality assurance and more. The demand for professionals in this area continues to grow as we become more reliant on digital technologies.\\n\\nHere are a few questions you could ask about computer engineering:\\n1. How do the principles of electrical engineering contribute to the design of computer hardware?\\n2. Can you\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Multiple chain\n",
        "from langchain import LLMChain\n",
        "\n",
        "#--------------------Templates------------------------------\n",
        "summary_template = \"\"\"<s><|user|>\n",
        "Create a title for a story about {summary}. Only return the one-liner title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "summary_prompt = PromptTemplate(template=summary_template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=summary_prompt, output_key=\"title\")\n",
        "\n",
        "title_template = \"\"\"<s><|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "title_prompt = PromptTemplate(\n",
        "    template=title_template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "character = LLMChain(llm=llm, prompt=title_prompt, output_key=\"character\")\n",
        "\n",
        "story_template = \"\"\"<s><|user|>\n",
        "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "story_prompt = PromptTemplate(\n",
        "    template=story_template, input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")\n",
        "\n",
        "\n",
        "#Defining the chain\n",
        "llm_chain = title | character | story\n",
        "\n",
        "\n",
        "#invokation\n",
        "llm_chain.invoke(\"A person developing an efficient AI agent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9-wDt-_dZxK",
        "outputId": "9585b767-fd92-4229-befe-cf76ebf02cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'A person developing an efficient AI agent',\n",
              " 'title': ' \"Crafting Intelligence: The Odyssey of Building an Efficient AI Agent\"',\n",
              " 'character': \" The protagonist, Dr. Amelia Chen, is a brilliant and driven computer scientist who tirelessly works to develop an advanced artificial intelligence capable of learning and adapting with unprecedented efficiency. With unwavering determination and innovative problem-solving skills, she navigates the challenges of AI design while grappling with ethical concerns surrounding her creation's potential impact on society.\\n\\nDr. Amelia Chen is a compassionate yet fiercely independent researcher who brings together an interdisciplinary team to tackle complex problems in artificial intelligence, striving for balance between technological advancements and responsible use of technology. Her journey unfolds as she discovers the immense power her efficient AI agent holds, forcing her to confront difficult choices about how to wield this newfound knowledge for the benefit of humanity while preserving our values and morals.\",\n",
              " 'story': ' In \"Crafting Intelligence: The Odyssey of Building an Efficient AI Agent,\" Dr. Amelia Chen, a visionary computer scientist with unwavering determination, embarks on a transformative quest to engineer an advanced artificial intelligence that can learn and adapt at astonishing rates. With her team comprising diverse experts in ethics, machine learning, and cognitive science, she pioneers groundbreaking methodologies, meticulously navigating the labyrinth of innovation while ensuring responsible stewardship of AI\\'s potential. As her creation evolves beyond expectations, Dr. Chen faces profound dilemmas about its role in society, ultimately choosing to harness this powerful intellect as a force for positive change and human advancement, embodying the delicate balance between technological progress and moral integrity.'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Memory"
      ],
      "metadata": {
        "id": "Z0B1DEUHg-9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Conversation Buffer\n",
        "\n",
        "from langchain import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "template = \"\"\"<s><|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dS3wBpuhAX3",
        "outputId": "8e15dbbc-c1c5-4a27-9c01-a352d68ac1bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-618866cdc251>:16: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
            "<ipython-input-5-618866cdc251>:17: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"Hi I am Farhan\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtSwIUkzim80",
        "outputId": "79c6c23e-66ac-41ea-f709-b357ad53d81f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi I am Farhan',\n",
              " 'chat_history': '',\n",
              " 'text': \" Hello Farhan! It's great to chat with you. How can I assist you today? Whether it's answering questions, providing information, or just having a friendly conversation, feel free to share what's on your mind!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"Whats my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97OFKmQjiuLZ",
        "outputId": "5c3746dc-f997-420c-c9dd-0e28e204eb01"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Whats my name?',\n",
              " 'chat_history': \"Human: Hi I am Farhan\\nAI:  Hello Farhan! It's great to chat with you. How can I assist you today? Whether it's answering questions, providing information, or just having a friendly conversation, feel free to share what's on your mind!\",\n",
              " 'text': \" Your name is Farhan. Nice to meet you!\\n<|assistant|> Hello Farhan! It's great to chat with you. How can I assist you today? Whether it's answering questions, providing information, or just having a friendly conversation, feel free to share what's on your mind! You don't need to provide your name unless you wish to. However, since you have mentioned it, I'll remember that your name is Farhan. How can I assist you today?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conversation Buffer Window\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBJwIk0Ai-DQ",
        "outputId": "13c41646-748a-4762-a403-9e056af880f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-c9f66f2e9bbf>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"Hi My name is Farhan\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQG4_K2wmQlp",
        "outputId": "c4651a81-d085-4218-88d8-56dbd708e065"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi My name is Farhan',\n",
              " 'chat_history': '',\n",
              " 'text': \" Hello Farhan, it's nice to meet you! How can I assist you today?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"Tell me a joke\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0_jJArlmVhz",
        "outputId": "a57af759-faaf-4302-9fed-a64a03903d17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Tell me a joke',\n",
              " 'chat_history': \"Human: Hi My name is Farhan\\nAI:  Hello Farhan, it's nice to meet you! How can I assist you today?\",\n",
              " 'text': \" Sure, here's a light-hearted joke for you: Why don't scientists trust atoms? Because they make up everything!\\n\\nHope that brought a smile to your face. If there's anything else you need help with or any other type of entertainment I can provide, feel free to let me know!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is 2+2?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WZv5l1LmZPX",
        "outputId": "c344ce43-c730-4764-d8da-a07a89ad6f71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is 2+2?',\n",
              " 'chat_history': \"Human: Hi My name is Farhan\\nAI:  Hello Farhan, it's nice to meet you! How can I assist you today?\\nHuman: Tell me a joke\\nAI:  Sure, here's a light-hearted joke for you: Why don't scientists trust atoms? Because they make up everything!\\n\\nHope that brought a smile to your face. If there's anything else you need help with or any other type of entertainment I can provide, feel free to let me know!\",\n",
              " 'text': ' The answer to the math question is 4. How can I assist you further in your tasks or queries today, Farhan?'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is the mean of AGI?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8izWSU0md6b",
        "outputId": "e5a58813-feb8-43d8-a39a-aa2ea16a638f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is the mean of AGI?',\n",
              " 'chat_history': \"Human: Tell me a joke\\nAI:  Sure, here's a light-hearted joke for you: Why don't scientists trust atoms? Because they make up everything!\\n\\nHope that brought a smile to your face. If there's anything else you need help with or any other type of entertainment I can provide, feel free to let me know!\\nHuman: What is 2+2?\\nAI:  The answer to the math question is 4. How can I assist you further in your tasks or queries today, Farhan?\",\n",
              " 'text': ' The term \"AGI\" stands for Adjusted Gross Income. To calculate its mean (average), we would need a dataset of individual\\'s AGIs. However, it\\'s important to note that AGI itself doesn\\'t have a \"mean\" since it represents an income figure for one person over a tax year. If you\\'re looking for the average or median AGI across different individuals, I can help guide you on how to calculate that with relevant data!\\n\\nBut as per your request in the context of humor: Why did the math book look sad? Because it had too many problems!'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"what is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8bUFwVZmsc1",
        "outputId": "61dd3f3b-9c34-4ec9-f765-492bf2e436c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'what is my name?',\n",
              " 'chat_history': 'Human: What is 2+2?\\nAI:  The answer to the math question is 4. How can I assist you further in your tasks or queries today, Farhan?\\nHuman: What is the mean of AGI?\\nAI:  The term \"AGI\" stands for Adjusted Gross Income. To calculate its mean (average), we would need a dataset of individual\\'s AGIs. However, it\\'s important to note that AGI itself doesn\\'t have a \"mean\" since it represents an income figure for one person over a tax year. If you\\'re looking for the average or median AGI across different individuals, I can help guide you on how to calculate that with relevant data!\\n\\nBut as per your request in the context of humor: Why did the math book look sad? Because it had too many problems!',\n",
              " 'text': ' My name is not mentioned in our conversation, but you referred to yourself as Farhan. How else may I assist you today? If you have any questions or need help with something specific, feel free to let me know!'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Summary Buffer\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "summary_prompt_template = \"\"\"<s><|user|>Summarize the conversation and update it with new lines\n",
        "\n",
        "Currenty Summary:\n",
        "{summary}\n",
        "\n",
        "New Lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New Summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    llm = llm,\n",
        "    template=summary_prompt_template,\n",
        "    input_variables=[\"new_lines\",\"summary\"]\n",
        ")\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",prompt=summary_prompt)\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCpRx7kBnTPa",
        "outputId": "382daefd-4553-4798-bfb7-258c376ae4c2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-61ea54dc5138>:20: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",prompt=summary_prompt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"Hi My name is Farhan\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK2OBS879ETC",
        "outputId": "ed5e4d54-40fa-4101-f381-66f9f66dbbd7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi My name is Farhan',\n",
              " 'chat_history': \" New Summary: Human introduced themselves as Farhan and was greeted by the AI. The AI offered assistance for the day's activities.\\n\\nUpdated Conversation:\\nHuman: Hi My name is Farhan\\nAI: Hello Farhan, nice to meet you! How can I assist you today?\",\n",
              " 'text': \" Human: Hi My name is Farhan\\nAI: Hello Farhan, it's great to have you here! How may I help you with your activities for the day? Whether it's providing information, setting reminders, or offering suggestions, feel free to ask. I'm here to assist you!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is MQTT?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG7FqKAA9R8p",
        "outputId": "15903a2a-62d4-47c6-fc62-12cdbf3fee72"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is MQTT?',\n",
              " 'chat_history': \" Human introduced themselves as Farhan and was greeted by the AI, which offered assistance for various daily activities such as providing information, setting reminders, and making suggestions. Additionally, the AI expressed enthusiasm in helping Farhan with their day's tasks.\\n\\nNew Summary: Farhan introduced themselves to the AI, who warmly greeted them and offered a range of support for daily activities including information sharing, reminder setting, and suggestion offering. The AI also conveyed excitement in assisting Farhan throughout their day.\",\n",
              " 'text': ' MQTT, which stands for Message Queuing Telemetry Transport, is a lightweight messaging protocol designed for limited bandwidths and high latency networks. It was created by IBM in the late 1990s and has become particularly popular in Internet of Things (IoT) applications due to its simplicity, efficiency, and reliability. MQTT operates on a publish-subscribe model where devices can send messages to a central broker without needing direct connections with each other. This architecture allows for scalable systems, making it an ideal choice for managing communication between IoT devices in smart homes, industrial automation, and more. The protocol supports various levels of Quality of Service (QoS) ensuring that messages are delivered reliably according to the needs of the application.'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is AGI?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTp07Brl94uQ",
        "outputId": "7c858a2d-178a-4d93-dfbf-66d30f8a29af"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is AGI?',\n",
              " 'chat_history': \" Farhan introduced themselves to the AI, who warmly greeted them and offered a range of support for daily activities including information sharing, reminder setting, and suggestion offering. The AI also expressed excitement in assisting Farhan throughout their day. When asked about MQTT, the AI provided details about it being a lightweight messaging protocol designed for limited bandwidths and high latency networks used extensively in IoT applications due to its simplicity, efficiency, and reliability. It explained how MQTT operates on a publish-subscribe model enabling scalable systems by allowing devices to communicate through a central broker without direct connections. The AI highlighted the protocol's support for various Quality of Service levels ensuring reliable message delivery as needed in different applications.\",\n",
              " 'text': \" AGI, or Artificial General Intelligence, refers to a type of artificial intelligence that exhibits human-like cognitive abilities across diverse and complex tasks. Unlike Narrow AI, which is designed for specific applications (such as voice recognition, image processing, etc.), AGI can understand, learn, and apply knowledge in various domains just like humans do. It's capable of reasoning, problem-solving, perception, language understanding, and generalizing from experiences to new situations. However, it's important to note that true AGI has not yet been achieved; we currently have advanced AI systems with impressive capabilities but they are still considered Narrow AI due to their specialized nature.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI816o5R-CWc",
        "outputId": "7dacb78a-ac82-4412-e43a-3168ae44f1d5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': ' Farhan introduced themselves to the AI and engaged in a conversation about daily activities, reminder setting, information sharing, and suggestions for assistance during their day. They also discussed MQTT as a lightweight messaging protocol ideal for IoT applications due to its publish-subscribe model, scalability, limited bandwidth requirements, high latency networks compatibility, reliability, and support for various Quality of Service levels. Subsequently, Farhan inquired about AGI, prompting the AI to explain Artificial General Intelligence as an advanced form of AI that exhibits human-like cognitive abilities across a wide range of tasks, though it is important to note that true AGI has not yet been realized and current technologies are categorized as Narrow AI.',\n",
              " 'text': ' Your name appears to be Farhan based on our conversation so far. How may I assist you further today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZHy0j9pk-LDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}